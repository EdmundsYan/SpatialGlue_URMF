preprocess.py:
import os
import scipy
import anndata
import sklearn
import torch
import random
import numpy as np
import scanpy as sc
import pandas as pd
from typing import Optional
import scipy.sparse as sp
from torch.backends import cudnn
from scipy.sparse import coo_matrix
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import kneighbors_graph 

def adjacent_matrix_preprocessing(adata_omics1, adata_omics2):
    """处理空间图和特征图的邻接矩阵，进行标准化并转化为稀疏矩阵"""
    
    # ===================== 构建空间图 =====================
    adj_spatial_omics1 = adata_omics1.uns['adj_spatial']
    adj_spatial_omics2 = adata_omics2.uns['adj_spatial']
    
    # 将空间图从 DataFrame 转为稀疏矩阵
    adj_spatial_omics1 = transform_adjacent_matrix(adj_spatial_omics1)
    adj_spatial_omics2 = transform_adjacent_matrix(adj_spatial_omics2)

    adj_spatial_omics1 = adj_spatial_omics1 + adj_spatial_omics1.T
    adj_spatial_omics1 = np.where(adj_spatial_omics1 > 1, 1, adj_spatial_omics1)
    adj_spatial_omics2 = adj_spatial_omics2 + adj_spatial_omics2.T
    adj_spatial_omics2 = np.where(adj_spatial_omics2 > 1, 1, adj_spatial_omics2)

    adj_spatial_omics1 = preprocess_graph(adj_spatial_omics1)  # 进行标准化
    adj_spatial_omics2 = preprocess_graph(adj_spatial_omics2)

    # ===================== 构建特征图 =====================
    adj_feature_omics1 = adata_omics1.obsm['adj_feature'].copy().toarray()
    adj_feature_omics2 = adata_omics2.obsm['adj_feature'].copy().toarray()
    
    adj_feature_omics1 = adj_feature_omics1 + adj_feature_omics1.T
    adj_feature_omics1 = np.where(adj_feature_omics1 > 1, 1, adj_feature_omics1)
    adj_feature_omics2 = adj_feature_omics2 + adj_feature_omics2.T
    adj_feature_omics2 = np.where(adj_feature_omics2 > 1, 1, adj_feature_omics2)

    adj_feature_omics1 = preprocess_graph(adj_feature_omics1)
    adj_feature_omics2 = preprocess_graph(adj_feature_omics2)
    
    adj = {
        'adj_spatial_omics1': adj_spatial_omics1,
        'adj_spatial_omics2': adj_spatial_omics2,
        'adj_feature_omics1': adj_feature_omics1,
        'adj_feature_omics2': adj_feature_omics2,
    }

    return adj
def transform_adjacent_matrix(adjacent):
    """ 将邻接图从DataFrame转换为稀疏矩阵 """
    n_spot = adjacent['x'].max() + 1
    adj = coo_matrix((adjacent['value'], (adjacent['x'], adjacent['y'])), shape=(n_spot, n_spot))
    return adj

def preprocess_graph(adj):
    """ 预处理邻接矩阵，标准化并转换为稀疏矩阵 """
    adj = sp.coo_matrix(adj)
    adj_ = adj + sp.eye(adj.shape[0])  # 加上单位矩阵确保对角元素为1
    rowsum = np.array(adj_.sum(1))
    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())
    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()
    return sparse_mx_to_torch_sparse_tensor(adj_normalized)

def sparse_mx_to_torch_sparse_tensor(sparse_mx):
    """ 将稀疏矩阵转换为 PyTorch 的稀疏张量 """
    sparse_mx = sparse_mx.tocoo().astype(np.float32)
    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
    values = torch.from_numpy(sparse_mx.data)
    shape = torch.Size(sparse_mx.shape)
    return torch.sparse.FloatTensor(indices, values, shape)
# ================== UDR 计算模块 ==================
def cog_uncertainty_sample(mu1, var1, mu2, var2, sample_times=10):
    """ 计算模态不确定性：进行多次采样 """
    eps1 = torch.randn(sample_times, *mu1.shape).to(mu1.device)
    eps2 = torch.randn(sample_times, *mu2.shape).to(mu2.device)
    sample1 = mu1.unsqueeze(0) + torch.sqrt(var1.unsqueeze(0)) * eps1
    sample2 = mu2.unsqueeze(0) + torch.sqrt(var2.unsqueeze(0)) * eps2
    return sample1.permute(1, 0, 2), sample2.permute(1, 0, 2)

def cog_uncertainty_normal(cog_uncertainty_dict):
    """ 归一化不确定性 """
    sum_uncertainty = sum(cog_uncertainty_dict.values())
    for key in cog_uncertainty_dict:
        cog_uncertainty_dict[key] /= sum_uncertainty
    return cog_uncertainty_dict

def reparameterize(mu, logvar):
    """ VAE 采样：重参数化技巧 """
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# ================== 图构建 ==================
def construct_neighbor_graph(adata_omics1, adata_omics2, datatype='SPOTS', n_neighbors=3): 
    """ 构建邻接图，包括共享的 Spatial Graph 和 Feature Graph """

    # 设置空间图邻接参数
    if datatype in ['Stereo-CITE-seq', 'Spatial-epigenome-transcriptome']:
       n_neighbors = 6  

    # 共享空间邻接图
    cell_positions = adata_omics1.obsm['spatial']
    adj_spatial = construct_graph_by_coordinate(cell_positions, n_neighbors=n_neighbors)
    adata_omics1.uns['adj_spatial'] = adj_spatial
    adata_omics2.uns['adj_spatial'] = adj_spatial  

    # 计算 Feature Graph
    feature_graph_omics1, feature_graph_omics2 = construct_graph_by_feature(adata_omics1, adata_omics2)
    adata_omics1.obsm['adj_feature'], adata_omics2.obsm['adj_feature'] = feature_graph_omics1, feature_graph_omics2
    
    # 数据标准化
    adata_omics1 = clr_normalize_each_cell(adata_omics1)
    adata_omics2 = clr_normalize_each_cell(adata_omics2)

    data = {'adata_omics1': adata_omics1, 'adata_omics2': adata_omics2}
    return data

def construct_graph_by_feature(adata_omics1, adata_omics2, k=20, mode="connectivity", metric="correlation", include_self=False):
    """ 构建基于基因表达的 Feature Graph """
    feature_graph_omics1 = kneighbors_graph(adata_omics1.obsm['feat'], k, mode=mode, metric=metric, include_self=include_self)
    feature_graph_omics2 = kneighbors_graph(adata_omics2.obsm['feat'], k, mode=mode, metric=metric, include_self=include_self)
    return feature_graph_omics1, feature_graph_omics2

def construct_graph_by_coordinate(cell_position, n_neighbors=3):
    """ 构建基于空间坐标的 Spatial Graph """
    nbrs = NearestNeighbors(n_neighbors=n_neighbors+1).fit(cell_position)  
    _, indices = nbrs.kneighbors(cell_position)
    x = indices[:, 0].repeat(n_neighbors)
    y = indices[:, 1:].flatten()
    adj = pd.DataFrame(columns=['x', 'y', 'value'])
    adj['x'] = x
    adj['y'] = y
    adj['value'] = np.ones(x.size)
    return adj

# ================== 预处理模块 ==================
def lsi(adata: anndata.AnnData, n_components: int = 20, use_highly_variable: Optional[bool] = None, **kwargs):
    """ LSI 降维 """
    if use_highly_variable is None:
        use_highly_variable = "highly_variable" in adata.var
    adata_use = adata[:, adata.var["highly_variable"]] if use_highly_variable else adata
    X = tfidf(adata_use.X)
    X_norm = sklearn.preprocessing.Normalizer(norm="l1").fit_transform(X)
    X_norm = np.log1p(X_norm * 1e4)
    X_lsi = sklearn.utils.extmath.randomized_svd(X_norm, n_components, **kwargs)[0]
    X_lsi -= X_lsi.mean(axis=1, keepdims=True)
    X_lsi /= X_lsi.std(axis=1, ddof=1, keepdims=True)
    adata.obsm["X_lsi"] = X_lsi[:,1:]

def tfidf(X):
    """ TF-IDF 归一化 """
    idf = X.shape[0] / X.sum(axis=0)
    if scipy.sparse.issparse(X):
        tf = X.multiply(1 / X.sum(axis=1))
        return tf.multiply(idf)
    else:
        tf = X / X.sum(axis=1, keepdims=True)
        return tf * idf   

def pca(adata, use_reps=None, n_comps=10):
    """ PCA 降维 """
    from sklearn.decomposition import PCA
    pca = PCA(n_components=n_comps)
    if use_reps is not None:
        feat_pca = pca.fit_transform(adata.obsm[use_reps])
    else:
        feat_pca = pca.fit_transform(adata.X.toarray() if isinstance(adata.X, sp.csc_matrix) else adata.X)
    return feat_pca

def clr_normalize_each_cell(adata, inplace=True):
    """ 归一化每个细胞的表达数据 """
    def seurat_clr(x):
        s = np.sum(np.log1p(x[x > 0]))
        exp = np.exp(s / len(x))
        return np.log1p(x / exp)

    if not inplace:
        adata = adata.copy()

    adata.X = np.apply_along_axis(
        seurat_clr, 1, (adata.X.A if scipy.sparse.issparse(adata.X) else np.array(adata.X))
    )
    return adata    

# ================== 随机性控制 ==================
def fix_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    cudnn.deterministic = True
    cudnn.benchmark = False

util1.py:
import os
import pickle
import numpy as np
import scanpy as sc
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from .preprocess import pca

# ========== Clustering with Mclust ==========
def mclust_R(adata, num_cluster, modelNames='EEE', used_obsm='emb_pca', random_seed=2020):
    """
    Clustering using the mclust algorithm.
    The parameters are the same as those in the R package mclust.
    """
    np.random.seed(random_seed)
    import rpy2.robjects as robjects
    robjects.r.library("mclust")

    import rpy2.robjects.numpy2ri
    rpy2.robjects.numpy2ri.activate()
    r_random_seed = robjects.r['set.seed']
    r_random_seed(random_seed)
    rmclust = robjects.r['Mclust']
    
    res = rmclust(rpy2.robjects.numpy2ri.numpy2rpy(adata.obsm[used_obsm]), num_cluster, modelNames)
    mclust_res = np.array(res[-2])

    adata.obs['mclust'] = mclust_res.astype('int').astype('category')
    return adata

# ========== Modified Clustering (Supports EURO & VAE) ==========
def clustering(adata, n_clusters=7, key='emb', add_key='SpatialGlue', method='mclust', 
               start=0.1, end=3.0, increment=0.01, use_pca=False, n_comps=20, use_vae=False):
    """
    Spatial clustering based on latent representation with EURO + VAE support.

    Parameters
    ----------
    adata : anndata
        AnnData object of scanpy package.
    n_clusters : int
        Number of clusters.
    key : string
        The key of the input representation in adata.obsm.
    method : string
        Clustering tool: 'mclust', 'leiden', 'louvain'.
    start : float
        Start value for searching resolution (for leiden/louvain).
    end : float 
        End value for searching resolution (for leiden/louvain).
    increment : float
        Step size for increasing resolution.
    use_pca : bool
        Whether to use PCA for dimension reduction.
    use_vae : bool
        Whether to use a VAE-based latent space.

    Returns
    -------
    None.
    """
    
    if use_pca:
       adata.obsm[key + '_pca'] = pca(adata, use_reps=key, n_comps=n_comps)
    
    rep_key = key + '_vae' if use_vae else key
    rep_key = rep_key + '_pca' if use_pca else rep_key

    if method == 'mclust':
       adata = mclust_R(adata, used_obsm=rep_key, num_cluster=n_clusters)
       adata.obs[add_key] = adata.obs['mclust']
       
    elif method in ['leiden', 'louvain']:
       res = search_res(adata, n_clusters, use_rep=rep_key, method=method, start=start, end=end, increment=increment)
       if method == 'leiden':
           sc.tl.leiden(adata, random_state=0, resolution=res)
           adata.obs[add_key] = adata.obs['leiden']
       elif method == 'louvain':
           sc.tl.louvain(adata, random_state=0, resolution=res)
           adata.obs[add_key] = adata.obs['louvain']

# ========== Adaptive Resolution Search (Supports VAE) ==========
def search_res(adata, n_clusters, method='leiden', use_rep='emb', start=0.1, end=3.0, increment=0.01):
    """
    Search for optimal resolution parameter.

    Parameters
    ----------
    adata : anndata
    n_clusters : int
    method : string
    use_rep : string
    start : float
    end : float 
    increment : float

    Returns
    -------
    res : float
    """
    print('Searching resolution...')
    label = 0
    sc.pp.neighbors(adata, n_neighbors=50, use_rep=use_rep)
    for res in sorted(list(np.arange(start, end, increment)), reverse=True):
        if method == 'leiden':
           sc.tl.leiden(adata, random_state=0, resolution=res)
           count_unique = len(pd.DataFrame(adata.obs['leiden']).leiden.unique())
        elif method == 'louvain':
           sc.tl.louvain(adata, random_state=0, resolution=res)
           count_unique = len(pd.DataFrame(adata.obs['louvain']).louvain.unique()) 

        print(f'resolution={res}, cluster number={count_unique}')
        if count_unique == n_clusters:
            label = 1
            break

    assert label == 1, "Resolution not found. Try a bigger range or smaller step."   
    return res     

# ========== EURO Mechanism: Adjust Weight Based on Uncertainty ==========
def apply_euro_weight(alpha, cog_uncertainty_dict):
    """
    Adjusts weight values based on the uncertainty from UDR.

    Parameters
    ----------
    alpha : np.array
        Initial weight values.
    cog_uncertainty_dict : dict
        Uncertainty values for RNA and Protein.

    Returns
    -------
    adjusted_alpha : np.array
    """
    weight_rna = 1 / (1 + cog_uncertainty_dict['protein'])
    weight_protein = 1 / (1 + cog_uncertainty_dict['rna'])

    adjusted_alpha = np.stack([alpha[:, 0] * weight_rna, alpha[:, 1] * weight_protein], axis=1)
    return adjusted_alpha / np.sum(adjusted_alpha, axis=1, keepdims=True)

# ========== Enhanced Weight Visualization (Supports EURO) ==========
def plot_weight_value(alpha, label, cog_uncertainty_dict=None, modality1='mRNA', modality2='protein'):
    """
    Plot weight values with EURO uncertainty correction.

    Parameters
    ----------
    alpha : np.array
    label : list
    cog_uncertainty_dict : dict
    modality1 : string
    modality2 : string

    Returns
    -------
    None.
    """
    if cog_uncertainty_dict:
        alpha = apply_euro_weight(alpha, cog_uncertainty_dict)

    df = pd.DataFrame(columns=[modality1, modality2, 'label'])  
    df[modality1], df[modality2] = alpha[:, 0], alpha[:, 1]
    df['label'] = label
    df = df.set_index('label').stack().reset_index()
    df.columns = ['label_SpatialGlue', 'Modality', 'Weight value']

    ax = sns.violinplot(data=df, x='label_SpatialGlue', y='Weight value', hue="Modality",
                        split=True, inner="quart", linewidth=1)
    ax.set_title(modality1 + ' vs ' + modality2)

    plt.tight_layout()
    plt.show()

model.py:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module

# ========== VAE + GCN + EURO + UDR 整合模型 ==========
class EncoderVAE(Module):
    """\
    VAE Encoder for a single modality.

    Parameters
    ----------
    in_feat: int
        Dimension of input features.
    latent_dim: int
        Dimension of latent space.
    
    Returns
    -------
    mu : Mean of latent space.
    logvar : Log variance of latent space.
    """
    
    def __init__(self, in_feat, latent_dim):
        super(EncoderVAE, self).__init__()
        self.fc_mu = nn.Linear(in_feat, latent_dim)
        self.fc_logvar = nn.Linear(in_feat, latent_dim)
    
    def forward(self, x):
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

def reparameterize(mu, logvar):
    """ VAE 重参数化 """
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

class Encoder_overall(Module):
    """\
    Overall encoder with VAE and GCN.

    Returns
    -------
    Dictionary with latent representations and uncertainty estimates.
    """
    
    def __init__(self, dim_in_omics1, dim_out_omics1, dim_in_omics2, dim_out_omics2, dropout=0.0, act=F.relu):
        super(Encoder_overall, self).__init__()
        
        # Omics-specific encoders
        self.encoder_omics1 = Encoder(dim_in_omics1, dim_out_omics1)
        self.encoder_omics2 = Encoder(dim_in_omics2, dim_out_omics2)

        # VAE encoders
        self.vae_encoder_omics1 = EncoderVAE(dim_out_omics1, dim_out_omics1)
        self.vae_encoder_omics2 = EncoderVAE(dim_out_omics2, dim_out_omics2)

        # Decoders
        self.decoder_omics1 = Decoder(dim_out_omics1, dim_in_omics1)
        self.decoder_omics2 = Decoder(dim_out_omics2, dim_in_omics2)
        
        # Attention layers
        self.atten_omics1 = AttentionLayer(dim_out_omics1, dim_out_omics1)
        self.atten_omics2 = AttentionLayer(dim_out_omics2, dim_out_omics2)
        self.atten_cross = AttentionLayer(dim_out_omics1, dim_out_omics2)
    
    def forward(self, features_omics1, features_omics2, adj_spatial, adj_feature_omics1, adj_feature_omics2):
        
        # GCN encoder processing
        emb_latent_spatial_omics1 = self.encoder_omics1(features_omics1, adj_spatial)  
        emb_latent_spatial_omics2 = self.encoder_omics2(features_omics2, adj_spatial)

        emb_latent_feature_omics1 = self.encoder_omics1(features_omics1, adj_feature_omics1)
        emb_latent_feature_omics2 = self.encoder_omics2(features_omics2, adj_feature_omics2)

        # Within-modality attention
        emb_latent_omics1, alpha_omics1 = self.atten_omics1(emb_latent_spatial_omics1, emb_latent_feature_omics1)
        emb_latent_omics2, alpha_omics2 = self.atten_omics2(emb_latent_spatial_omics2, emb_latent_feature_omics2)

        # VAE encoding
        mu_omics1, logvar_omics1 = self.vae_encoder_omics1(emb_latent_omics1)
        mu_omics2, logvar_omics2 = self.vae_encoder_omics2(emb_latent_omics2)
        z_omics1 = reparameterize(mu_omics1, logvar_omics1)
        z_omics2 = reparameterize(mu_omics2, logvar_omics2)

        # Between-modality attention
        emb_latent_combined, alpha_omics_1_2 = self.atten_cross(z_omics1, z_omics2)

        # Reconstruction via decoders
        emb_recon_omics1 = self.decoder_omics1(emb_latent_combined, adj_spatial)
        emb_recon_omics2 = self.decoder_omics2(emb_latent_combined, adj_spatial)

        results = {
            'mu_omics1': mu_omics1, 'logvar_omics1': logvar_omics1, 'z_omics1': z_omics1,
            'mu_omics2': mu_omics2, 'logvar_omics2': logvar_omics2, 'z_omics2': z_omics2,
            'emb_recon_omics1': emb_recon_omics1, 'emb_recon_omics2': emb_recon_omics2,
            'alpha_omics1': alpha_omics1, 'alpha_omics2': alpha_omics2, 'alpha': alpha_omics_1_2
        }
        
        return results

# ========== Standard GCN Encoder ==========
class Encoder(Module): 
    def __init__(self, in_feat, out_feat, dropout=0.0, act=F.relu):
        super(Encoder, self).__init__()
        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))
        self.reset_parameters()
    
    def reset_parameters(self):
        torch.nn.init.xavier_uniform_(self.weight)
    
    def forward(self, feat, adj):
        x = torch.mm(feat, self.weight)
        x = torch.spmm(adj, x)
        return x

# ========== Standard GCN Decoder ==========
class Decoder(Module):
    def __init__(self, in_feat, out_feat, dropout=0.0, act=F.relu):
        super(Decoder, self).__init__()
        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))
        self.reset_parameters()
    
    def reset_parameters(self):
        torch.nn.init.xavier_uniform_(self.weight)
    
    def forward(self, feat, adj):
        x = torch.mm(feat, self.weight)
        x = torch.spmm(adj, x)
        return x                  

# ========== Attention Layer ==========
class AttentionLayer(Module):
    def __init__(self, in_feat, out_feat, dropout=0.0, act=F.relu):
        super(AttentionLayer, self).__init__()
        self.w_omega = Parameter(torch.FloatTensor(in_feat, out_feat))
        self.u_omega = Parameter(torch.FloatTensor(out_feat, 1))
        self.reset_parameters()
    
    def reset_parameters(self):
        torch.nn.init.xavier_uniform_(self.w_omega)
        torch.nn.init.xavier_uniform_(self.u_omega)
        
    def forward(self, emb1, emb2):
        emb = torch.cat([torch.unsqueeze(emb1, dim=1), torch.unsqueeze(emb2, dim=1)], dim=1)
        v = F.tanh(torch.matmul(emb, self.w_omega))
        vu = torch.matmul(v, self.u_omega)
        alpha = F.softmax(vu.squeeze() + 1e-6)  
        emb_combined = torch.matmul(torch.transpose(emb,1,2), torch.unsqueeze(alpha, -1))
        return emb_combined.squeeze(), alpha      

train.py:
import torch
from tqdm import tqdm
import torch.nn.functional as F
from .model import Encoder_overall
from .preprocess import adjacent_matrix_preprocessing

class Train_SpatialGlue:
    def __init__(self, 
        data,
        datatype='SPOTS',
        device=torch.device('cpu'),
        random_seed=2022,
        learning_rate=0.0001,
        weight_decay=0.00,
        epochs=600, 
        dim_input=3000,
        dim_output=64,
        weight_factors=[1, 5, 1, 1, 1]  # 添加 KL 散度的权重因子
    ):
        """
        VAE + GCN + EURO + UDR 训练过程
        """
        self.data = data.copy()
        self.datatype = datatype
        self.device = device
        self.random_seed = random_seed
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.epochs = epochs
        self.dim_input = dim_input
        self.dim_output = dim_output
        self.weight_factors = weight_factors

        # 预处理邻接矩阵
        self.adata_omics1 = self.data['adata_omics1']
        self.adata_omics2 = self.data['adata_omics2']
        self.adj = adjacent_matrix_preprocessing(self.adata_omics1, self.adata_omics2)
        self.adj_spatial = self.adj['adj_spatial_omics1'].to(self.device)  # RNA 和 Protein 共享
        self.adj_feature_omics1 = self.adj['adj_feature_omics1'].to(self.device)
        self.adj_feature_omics2 = self.adj['adj_feature_omics2'].to(self.device)

        # 处理输入特征
        self.features_omics1 = torch.FloatTensor(self.adata_omics1.obsm['feat'].copy()).to(self.device)
        self.features_omics2 = torch.FloatTensor(self.adata_omics2.obsm['feat'].copy()).to(self.device)

        self.n_cell_omics1 = self.adata_omics1.n_obs
        self.n_cell_omics2 = self.adata_omics2.n_obs

        # 设置输入/输出维度
        self.dim_input1 = self.features_omics1.shape[1]
        self.dim_input2 = self.features_omics2.shape[1]
        self.dim_output1 = self.dim_output
        self.dim_output2 = self.dim_output

        # 适配不同数据类型
        if self.datatype == 'SPOTS':
            self.epochs = 600 
            self.weight_factors = [1, 5, 1, 1, 0.1]  # KL 权重因子
        elif self.datatype == 'Stereo-CITE-seq':
            self.epochs = 1500 
            self.weight_factors = [1, 10, 1, 10, 0.05]
        elif self.datatype == '10x':
            self.epochs = 200
            self.weight_factors = [1, 5, 1, 10, 0.05]
        elif self.datatype == 'Spatial-epigenome-transcriptome': 
            self.epochs = 1600
            self.weight_factors = [1, 5, 1, 1, 0.05]

    def train(self):
        self.model = Encoder_overall(self.dim_input1, self.dim_output1, self.dim_input2, self.dim_output2).to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), self.learning_rate, weight_decay=self.weight_decay)

        self.model.train()
        for epoch in tqdm(range(self.epochs)):
            self.model.train()
            results = self.model(
                self.features_omics1, self.features_omics2, self.adj_spatial, 
                self.adj_feature_omics1, self.adj_feature_omics2
            )
            
            # 计算 Reconstruction Loss
            self.loss_recon_omics1 = F.mse_loss(self.features_omics1, results['emb_recon_omics1'])
            self.loss_recon_omics2 = F.mse_loss(self.features_omics2, results['emb_recon_omics2'])
            
            # 计算 Correspondence Loss
            self.loss_corr_omics1 = F.mse_loss(results['z_omics1'], results['mu_omics1'])
            self.loss_corr_omics2 = F.mse_loss(results['z_omics2'], results['mu_omics2'])

            # 计算 KL 散度损失（VAE 正则化）
            kl_div_omics1 = -0.5 * torch.sum(1 + results['logvar_omics1'] - results['mu_omics1']**2 - torch.exp(results['logvar_omics1']))
            kl_div_omics2 = -0.5 * torch.sum(1 + results['logvar_omics2'] - results['mu_omics2']**2 - torch.exp(results['logvar_omics2']))
            self.loss_kl = (kl_div_omics1 + kl_div_omics2) / self.n_cell_omics1

            # 计算 UDR 认知不确定性
            self.cog_uncertainty_dict = {
                'rna': torch.var(results['mu_omics1'], dim=0).mean(),
                'protein': torch.var(results['mu_omics2'], dim=0).mean()
            }

            # 计算最终损失
            loss = (self.weight_factors[0] * self.loss_recon_omics1 
                    + self.weight_factors[1] * self.loss_recon_omics2 
                    + self.weight_factors[2] * self.loss_corr_omics1 
                    + self.weight_factors[3] * self.loss_corr_omics2
                    + self.weight_factors[4] * self.loss_kl)  # KL 散度

            # EURO 机制调整梯度
            loss = self.apply_euro_gradient_adjustment(loss, self.cog_uncertainty_dict)

            self.optimizer.zero_grad()
            loss.backward() 
            self.optimizer.step()
        
        print("Model training finished!\n")    
    
        with torch.no_grad():
            self.model.eval()
            results = self.model(self.features_omics1, self.features_omics2, self.adj_spatial, self.adj_feature_omics1, self.adj_feature_omics2)
 
        emb_omics1 = F.normalize(results['z_omics1'], p=2, eps=1e-12, dim=1)  
        emb_omics2 = F.normalize(results['z_omics2'], p=2, eps=1e-12, dim=1)
        emb_combined = F.normalize(results['emb_latent_combined'], p=2, eps=1e-12, dim=1)
        
        output = {
            'z_omics1': emb_omics1.detach().cpu().numpy(),
            'z_omics2': emb_omics2.detach().cpu().numpy(),
            'SpatialGlue': emb_combined.detach().cpu().numpy(),
            'alpha_omics1': results['alpha_omics1'].detach().cpu().numpy(),
            'alpha_omics2': results['alpha_omics2'].detach().cpu().numpy(),
            'alpha': results['alpha'].detach().cpu().numpy()
        }
        
        return output

    def apply_euro_gradient_adjustment(self, loss, cog_uncertainty_dict):
        """
        EURO 机制：根据 UDR 计算不确定性，自适应调整梯度。
        """
        coeff_rna = 1 + 0.1 * cog_uncertainty_dict['protein']
        coeff_protein = 1 + 0.1 * cog_uncertainty_dict['rna']

        for name, params in self.model.named_parameters():
            if 'encoder_omics1' in name:
                params.grad *= coeff_rna
            if 'encoder_omics2' in name:
                params.grad *= coeff_protein
        
        return loss